{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3810d3-a909-45b3-acc7-1f4faea4f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1.##Q1.\n",
    "\n",
    "Batch normalization is a technique used in Artificial Neural Networks (ANNs) to improve the training process and overall performance of the network. It aims to address the issue of internal covariate shift, which refers to the change in the distribution of layer inputs during training. By normalizing the inputs within each mini-batch, batch normalization helps stabilize and speed up the training process.\n",
    "\n",
    "The benefits of using batch normalization during training include:\n",
    "\n",
    "Reduced internal covariate shift: Batch normalization ensures that the mean activation of each layer remains close to zero, and the standard deviation is close to one. This helps in reducing the internal covariate shift, which in turn leads to faster and more stable convergence during training.\n",
    "\n",
    "Improved gradient flow: Batch normalization normalizes the inputs for each mini-batch, which helps in reducing the dependence of gradients on the scale of the parameters or the initial learning rate. This improves the flow of gradients through the network, allowing for better and more efficient training.\n",
    "\n",
    "Regularization effect: Batch normalization introduces a slight regularization effect by adding noise to the network during training. This noise acts as a regularizer and helps in reducing overfitting, allowing the network to generalize better to unseen data.\n",
    "\n",
    "Better handling of different scales and distributions: Batch normalization normalizes the inputs within each mini-batch, making the network less sensitive to the scale and distribution of the input data. This enables the network to handle inputs with different scales and distributions more effectively.\n",
    "\n",
    "The working principle of batch normalization involves two main steps: normalization and learnable parameters.\n",
    "\n",
    "Normalization step: In this step, the inputs to a layer are normalized to have zero mean and unit variance. For each mini-batch during training, the mean and variance of the inputs are computed. Then, the inputs are subtracted by the mean and divided by the square root of the variance. This normalization step ensures that the inputs to the subsequent layer have similar distributions, regardless of the scale and distribution of the inputs.\n",
    "\n",
    "Learnable parameters: In addition to the normalization step, batch normalization introduces learnable parameters to the network. These parameters include a scale parameter (gamma) and a shift parameter (beta). These parameters allow the network to learn the optimal scale and shift for the normalized inputs. The scale parameter scales the normalized inputs, and the shift parameter adds a bias term. These parameters are learned during the training process using backpropagation.\n",
    "\n",
    "During inference (testing or prediction phase), the learned mean and variance for each batch are used to normalize the inputs, ensuring consistency with the training process.\n",
    "\n",
    "Overall, batch normalization helps in addressing the internal covariate shift, improves gradient flow, adds regularization, and enhances the network's ability to handle inputs with different scales and distributions, resulting in more efficient and effective training of artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a734c1c-08ce-4f60-9271-5014034aae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.\n",
    "\n",
    "\n",
    "To demonstrate the impact of batch normalization, let's consider the MNIST dataset, which consists of grayscale images of handwritten digits. We'll implement a simple feedforward neural network using the PyTorch deep learning framework.\n",
    "\n",
    "First, we need to preprocess the dataset by normalizing the pixel values and splitting it into training and validation sets:\n",
    "    \n",
    "    \n",
    " import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Loading MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Splitting into training and validation sets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "Now, we can implement the feedforward neural network without using batch normalization:\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea31da-db0c-4fef-ab3e-695a384dcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3.\n",
    "\n",
    "Batch normalization is a technique commonly used in deep learning to improve the training and convergence of neural networks. It aims to normalize the inputs of each layer in a network by adjusting and scaling them. This normalization step helps to address the problem of internal covariate shift, where the distribution of inputs to each layer changes during training, making it difficult for the network to learn effectively.\n",
    "\n",
    "The working principle of batch normalization involves two main steps: normalization and scaling, along with learnable parameters for adaptation. Let's discuss each step in detail:\n",
    "\n",
    "Normalization:\n",
    "During training, batch normalization normalizes the input of each layer by subtracting the batch mean and dividing by the batch standard deviation. This process is applied independently to each dimension of the layer's input.\n",
    "Given a mini-batch of inputs, let's denote it as X, with dimensions (m, n), where m represents the batch size and n represents the number of features. The normalization step computes the mean and variance of the batch:\n",
    "\n",
    "μ_B = (1/m) * Σ(X)\n",
    "σ_B^2 = (1/m) * Σ((X - μ_B)^2)\n",
    "\n",
    "Here, μ_B is the mean of the batch and σ_B^2 is the variance of the batch. Next, batch normalization standardizes the inputs by subtracting the mean and dividing by the standard deviation:\n",
    "\n",
    "X_hat = (X - μ_B) / sqrt(σ_B^2 + ε)\n",
    "\n",
    "Here, ε is a small constant (e.g., 10^-8) added to the denominator for numerical stability.\n",
    "\n",
    "Scaling and Shifting:\n",
    "After normalization, the outputs are rescaled and shifted using learnable parameters. This step allows the network to learn an optimal scale and shift for each normalized feature. It introduces two learnable parameters per feature: a scaling parameter (γ) and a shifting parameter (β).\n",
    "Y = γ * X_hat + β\n",
    "\n",
    "The scaling parameter γ controls the standard deviation of the output, while the shifting parameter β controls the mean. These parameters are learned during training through backpropagation, and they allow the network to recover the representational power of the original layer.\n",
    "\n",
    "By incorporating normalization and learnable parameters, batch normalization helps stabilize and accelerate the training process. It reduces the internal covariate shift by ensuring that the distribution of inputs to each layer remains more consistent throughout training. Additionally, it has been observed that batch normalization acts as a regularizer, reducing the need for other regularization techniques like dropout.\n",
    "\n",
    "During inference, the normalization and scaling steps are applied in a slightly different manner. Instead of computing the batch mean and variance, the population mean and variance (estimated during training) are used for normalization. This ensures consistent behavior between training and inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e846560-3aee-4227-a332-355fbdd49c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.\n",
    "1.Sure! Let's choose the MNIST dataset, which is a widely used dataset for image classification tasks. MNIST consists of grayscale images of handwritten digits from 0 to 9. Each image is 28x28 pixels in size.\n",
    "\n",
    "To preprocess the MNIST dataset, we can perform the following steps:\n",
    "\n",
    "Load the Dataset:\n",
    "The MNIST dataset is readily available in various machine learning libraries, such as TensorFlow or PyTorch. You can download and load the dataset using the respective library's functions or use preloaded versions.\n",
    "\n",
    "Data Normalization:\n",
    "Normalize the pixel values of the images to a range between 0 and 1. This step helps in stabilizing the training process. Divide each pixel value by 255, as the grayscale values range from 0 to 255.\n",
    "\n",
    "Reshaping:\n",
    "Reshape the images from a 2D array (28x28 pixels) to a 1D array (784 pixels). This step is necessary to match the input dimensions expected by most deep learning models.\n",
    "\n",
    "One-Hot Encoding:\n",
    "Encode the class labels into categorical binary vectors using one-hot encoding. This step converts the class labels (0-9) into a binary vector representation, where each class is represented as a vector of zeros, except for the index corresponding to the class label, which is set to 1.\n",
    "\n",
    "Train-Test Split:\n",
    "Split the dataset into training and testing sets. Typically, a common split is to allocate around 80% of the data for training and 20% for testing. This ensures that the model is evaluated on unseen data during testing.\n",
    "\n",
    "Optional: Data Augmentation:\n",
    "If desired, you can perform data augmentation techniques such as random rotations, translations, or flips. Data augmentation helps in increasing the diversity of the training data, leading to better generalization of the model.\n",
    "\n",
    "These preprocessing steps ensure that the MNIST dataset is in a suitable format for training a deep learning model. Once preprocessed, you can feed the data into your model for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343aab-26c2-4798-940e-8c033d0b9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the architecture of the feedforward neural network\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 784  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset and apply preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the feedforward neural network\n",
    "model = FeedForwardNN(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total}%')\n",
    "    \n",
    "    \n",
    "    In this example, we define a simple feedforward neural network with one hidden layer. The input size is 784 (corresponding to the flattened 28x28 images), the hidden layer size is 128, and the output size is 10 (representing the number of classes in the MNIST dataset). The ReLU activation function is used between the layers. The model is trained using the Adam optimizer and cross-entropy loss function.\n",
    "\n",
    "We load the MNIST dataset using PyTorch's datasets module and preprocess the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255ad7c-9a05-4f17-aa7c-fe6092a0ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the architecture of the feedforward neural network\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 784  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset and apply preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the feedforward neural network\n",
    "model = FeedForwardNN(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total}%')\n",
    "In this modified code, the batch normalization layers are removed from the original code. The rest of the code remains the same. Now, the model is trained without using batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c81ec4-c320-43f8-9581-e68190bf3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the architecture of the feedforward neural network with batch normalization\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 784  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset and apply preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the feedforward neural network with batch normalization\n",
    "model = FeedForwardNN(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "# Testing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total}%')\n",
    "In this modified code, the batch normalization layer (nn.BatchNorm1d) is added after the first fully connected layer (self.fc1). This ensures that the inputs to the second fully connected layer (self.fc2) are normalized. The rest of the code remains the same, including the training loop and the testing phase. Now, the model includes batch normalization layers, and it will be trained and evaluated accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7ac45-eea5-4b9c-9120-409af238ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.Certainly! Let's compare the training and validation performance (accuracy and loss) between the models with and without batch normalization. Here's the modified code to include the performance comparison:\n",
    "\n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the architecture of the feedforward neural network\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, use_batch_norm):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        if self.use_batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 784  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset and apply preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model without batch normalization\n",
    "model_no_bn = FeedForwardNN(input_size, hidden_size, num_classes, use_batch_norm=False).to(device)\n",
    "\n",
    "# Model with batch normalization\n",
    "model_with_bn = FeedForwardNN(input_size, hidden_size, num_classes, use_batch_norm=True).to(device)\n",
    "\n",
    "# Define the loss function and optimizer for both models\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_no_bn = optim.Adam(model_no_bn.parameters(), lr=learning_rate)\n",
    "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for both models\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    model_no_bn.train()\n",
    "    model_with_bn.train()\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Training for model without batch normalization\n",
    "        optimizer_no_bn.zero_grad()\n",
    "        outputs_no_bn = model_no_bn(images)\n",
    "        loss_no_bn = criterion(outputs_no_bn, labels)\n",
    "        loss_no_bn.backward()\n",
    "        optimizer_no_bn.step()\n",
    "        \n",
    "        # Training for model with batch normalization\n",
    "        optimizer_with_bn.zero_grad()\n",
    "        outputs_with_bn = model_with_bn(images)\n",
    "        loss_with_bn = criterion(outputs_with_bn, labels)\n",
    "        loss_with_bn.backward()\n",
    "        optimizer_with_bn.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}]')\n",
    "            print(f'Loss without BN: {loss_no_bn.item():.4f}')\n",
    "            print(f'Loss with BN: {loss_with_bn.item():.4f}')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456daf3-017b-4e7b-8a21-4cf829cc2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.\n",
    "\n",
    "\n",
    "Batch normalization has several impacts on the training process and the performance of a neural network:\n",
    "\n",
    "Improved convergence speed: Batch normalization helps in stabilizing and accelerating the training process. By normalizing the input activations within each mini-batch, it reduces the internal covariate shift problem. This allows the network to converge faster and more efficiently during training.\n",
    "\n",
    "Increased generalization ability: Batch normalization acts as a regularizer by adding noise to the network activations. This noise, combined with the normalization, helps in reducing overfitting and improving the generalization ability of the model. It allows the network to perform better on unseen data.\n",
    "\n",
    "Mitigation of vanishing/exploding gradients: Batch normalization helps to alleviate the vanishing and exploding gradient problems. By normalizing the activations, it keeps them within a suitable range during backpropagation, which makes it easier for the gradients to flow and prevents them from becoming too small or too large.\n",
    "\n",
    "Reduction of dependence on initialization: Batch normalization makes the network less sensitive to the choice of initial weights. It helps in reducing the dependence on careful initialization, allowing the network to converge even with suboptimal initial weights.\n",
    "\n",
    "Higher learning rates: Batch normalization allows for the use of higher learning rates. It stabilizes the network by reducing the chances of extreme activation values, enabling faster convergence with larger learning rates.\n",
    "\n",
    "Smoother optimization landscape: By normalizing the activations, batch normalization leads to a smoother optimization landscape. This makes the loss surface more bowl-shaped, which facilitates faster convergence and avoids getting trapped in poor local optima.\n",
    "\n",
    "Overall, the inclusion of batch normalization in a neural network can lead to improved training stability, faster convergence, better generalization, and increased performance on both the training and validation datasets. It has become a widely used technique in deep learning due to its beneficial effects on the training process and the overall performance of neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb1d4e-58a3-4f6a-8abb-3cfc9364d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "part3.\n",
    "\n",
    "1.\n",
    "\n",
    "Certainly! Experimenting with different batch sizes can have an impact on the training dynamics and model performance. Here's an updated code that allows you to specify different batch sizes and observe their effects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae411f-9051-4f4d-94f9-575c843ad1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ffb31-e8a1-405b-bb4f-7f90497ac082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
